import kb_audio_boot  # output defaults + mic auto-detect (sets KINDERBOT_SR)
from kb_force_input import force_bind_input
force_bind_input()  # bind input mic only
import kb_input_bind  # picks & binds input mic
import kb_input_print  # prints bound input
import kb_input_bind  # pick/bind input mic before audio boot
from kb_capture import capture_phrase

import kinderbot_env  # loads ~/.config/kinderbot/.env
import os, sounddevice as sd
# sd.default.device=os.getenv("AUDIO_DEVICE") or None
# sd.default.samplerate=int(os.getenv("AUDIO_SR") or 48000)
# sd.default.channels=2
# sd.default.dtype="int16"
import os, sounddevice as sd
# sd.default.dtype="int16"; sd.default.device=(None, int(os.getenv("AUDIO_CARD") or -1)) if os.getenv("AUDIO_CARD") else None
import sounddevice as sd
# sd.default.dtype="int16"; sd.default.device=(None,int(__import__("os").getenv("AUDIO_OUT_ID") or -1)) if __import__("os").getenv("AUDIO_OUT_ID") else None
from kinderbot_behaviors import MotionPlanner, no_speech_fallback, SpeechGate, should_interrupt, send_motion
import warnings; warnings.filterwarnings("ignore", message="pkg_resources is deprecated")
import os, time, glob, subprocess, re, traceback
import numpy as np
import sounddevice as sd, soundfile as sf
import serial, webrtcvad
from collections import deque
from openai import OpenAI
from pathlib import Path
from dotenv import load_dotenv


DOTENV = Path.home() / ".config" / "kinderbot" / ".env"
load_dotenv(dotenv_path=DOTENV, override=True)

raw_key = os.getenv("OPENAI_API_KEY", "")
api_key = raw_key.strip()  # trims hidden \n or spaces

if not api_key or not api_key.startswith(("sk-", "sk-proj-")):
    raise RuntimeError(f"OPENAI_API_KEY missing/malformed (loaded from {DOTENV})")

client = OpenAI(api_key=api_key)
planner = MotionPlanner(home_every=(5,10))
speech_gate = SpeechGate()

# ===== settings =====
FAST_MODE     = True
PLAY_AUDIO    = True                                   # SPEAKER ON (PAM8302)
DEVICE_SR     = int(os.getenv("KINDERBOT_SR","48000")) # USB mic rate
FRAME_MS      = 20
PREROLL_S     = 0.20
SILENCE_S     = 0.35
MAX_UTT_S     = 2.20
MIN_SPEECH_S  = 0.25
ENERGY_GATE   = 0.002
SERIAL_PORT   = os.getenv("KINDERBOT_SERIAL", "/dev/ttyUSB0")
VOICE_MODEL   = os.getenv("KINDERBOT_TTS","openai")    # "openai" or "espeak"
OPENAI_VOICE  = os.getenv("KINDERBOT_VOICE","alloy")
MODEL_PLAN    = os.getenv("KINDERBOT_LLM","gpt-4o-mini")
# ====================

def load_dotenv():
    p = os.path.expanduser("~/.config/kinderbot/.env")
    if os.path.exists(p):
        for line in open(p):
            line=line.strip()
            if line and not line.startswith("#") and "=" in line:
                k,v=line.split("=",1); os.environ.setdefault(k.strip(), v.strip())
load_dotenv()

RECENT      = deque(maxlen=24)

# ---------- audio helpers ----------
def _resample_to_16k(int16_mono: np.ndarray, src_sr: int) -> np.ndarray:
    if src_sr == 16000: return int16_mono
    if src_sr % 16000 == 0:
        f = src_sr // 16000
        n = (len(int16_mono) // f) * f
        resh = int16_mono[:n].reshape(-1, f).astype(np.float32)
        y = resh.mean(axis=1)
        return np.clip(y, -32768, 32767).astype(np.int16)
    ratio = 16000.0 / float(src_sr)
    n_out = max(1, int(len(int16_mono) * ratio))
    x  = np.arange(len(int16_mono), dtype=np.float32)
    xi = np.linspace(0, len(int16_mono)-1, n_out, dtype=np.float32)
    y  = np.interp(xi, x, int16_mono.astype(np.float32))
    return np.clip(y, -32768, 32767).astype(np.int16)

def _rms_int16(x: np.ndarray) -> float:
    xf = x.astype(np.float32) / 32768.0
    return float(np.sqrt(np.mean(xf*xf) + 1e-12))

def capture_phrase(label:str, path16:str,
                   device_sr:int=DEVICE_SR, frame_ms:int=FRAME_MS,
                   preroll_s:float=PREROLL_S, silence_s:float=SILENCE_S,
                   max_s:float=MAX_UTT_S, min_speech_s:float=MIN_SPEECH_S) -> str:
    """Capture a phrase using WebRTC VAD (aggressive) + energy gate; save 16k WAV."""
    vad = webrtcvad.Vad(3)
    frame_dev = int(device_sr * frame_ms / 1000.0)
    preroll_frames   = int(preroll_s * 1000 / frame_ms)
    silence_frames   = int(silence_s * 1000 / frame_ms)
    min_speech_frames= int(min_speech_s * 1000 / frame_ms)

    ring, collected = [], []
    in_speech = False
    sil_cnt = 0
    speech_frames = 0
    total_frames  = 0

    print(f"[{label}] Waiting… (Ctrl+C to exit)")
    with sd.InputStream(samplerate=device_sr, channels=1, dtype="int16",
                        blocksize=frame_dev) as stream:
        while True:
            data,_ = stream.read(frame_dev)      # (N,1) int16
            mono   = data[:,0].copy()
            # 16k for VAD
            frame16 = _resample_to_16k(mono, device_sr)
            is_speech = vad.is_speech(frame16.tobytes(), 16000) and (_rms_int16(frame16) >= ENERGY_GATE)

            if not in_speech:
                ring.append(mono)
                if len(ring) > preroll_frames: ring.pop(0)
                if is_speech:
                    in_speech = True
                    collected.extend(ring); ring.clear()
                    collected.append(mono)
                    total_frames=1; speech_frames=1; sil_cnt=0
                    print(f"[{label}] speech start")
            else:
                collected.append(mono)
                total_frames += 1
                if is_speech:
                    speech_frames += 1; sil_cnt = 0
                else:
                    sil_cnt += 1
                dur_s = total_frames * frame_ms / 1000.0
                if sil_cnt >= silence_frames or dur_s >= max_s:
                    break

    if not collected or speech_frames < min_speech_frames:
        print(f"[{label}] too short/garbled (speech_frames={speech_frames}), ignoring.")
        return ""

    dev_audio = np.concatenate(collected, axis=0)
    y16 = _resample_to_16k(dev_audio, device_sr)
    sf.write(path16, y16, 16000, subtype="PCM_16")
    print(f"[{label}] captured {len(y16)/16000.0:.2f}s")
    time.sleep(0.1)
    return path16

def transcribe(path16:str) -> str:
    if not path16: return ""
    try:
        with open(path16, "rb") as f:
            tr = client.audio.transcriptions.create(
                model="gpt-4o-mini-transcribe",
                file=f,
                language="en"
            )
        txt = (getattr(tr, "text", "") or "").strip()
        return txt if re.search(r"[A-Za-z0-9]", txt) else ""
    except Exception as e:
        print("[stt err]", e); return ""

# ---------- planning ----------
PROMPT = (
"You are Laughbot. Return ONE funny joke when requested, with or without context to the input. "
"No preface, no quotes, no emojis, no meta. Vary joke styles; avoid clichés."
)

def plan_action(user_text: str) -> dict:
    try:
        r = client.responses.create(
            model=MODEL_PLAN,
            input=[{"role":"system","content":PROMPT},
                   {"role":"user","content":f"User said: {user_text}"}]
        )
        line = (getattr(r, "output_text", "") or "").strip()
    except Exception as e:
        print("[llm err]", e); line="Okay, let’s keep it snappy."
    RECENT.append(line[:120])
    return {"say": line}

# ---------- speak & serial ----------
def say_espeak(text):
    try: subprocess.run(["espeak-ng","-s","165","-p","45",text], check=False)
    except Exception as e: print("[espeak warn]", e)

def say_openai(text, outfile="tts.mp3", voice=OPENAI_VOICE):
    try:
        with client.audio.speech.with_streaming_response.create(
            model="gpt-4o-mini-tts", voice=voice, input=text
        ) as resp:
            resp.stream_to_file(outfile)
        subprocess.run(["mpg123","-q",outfile], check=False)
    except Exception as e:
        print("[tts err]", e); say_espeak(text)

def speak(text):
    if not PLAY_AUDIO:
        print("SAY:", text); return
    if VOICE_MODEL.lower().startswith("openai"):
        say_openai(text)
    else:
        say_espeak(text)

def _detect_ports():
    if os.path.exists(SERIAL_PORT): return [SERIAL_PORT]
    return sorted(glob.glob("/dev/ttyUSB*")+glob.glob("/dev/ttyACM*"))

def send_serial(lines):
    ports = _detect_ports()
    if not ports:
        print("[serial] no ports; skipping:", lines); return
    for p in ports:
        try:
            with serial.Serial(p, 115200, timeout=0.2) as ser:
                for line in lines:
                    ser.write((line+"\n").encode()); time.sleep(0.02)
            return
        except Exception as e:
            print(f"[serial warn] {p}: {e}")

def apply_plan(plan: dict):
    send_motion(lambda s: send_serial([s.strip()]), planner.next_move())
    # simple default flourish every line
    cmds = ["EMO:EXCITED","SPIN:260","SPIN_CCW 260","LED OFF"]
    send_serial(cmds)

# ---------- main ----------
def main():
    print("Kinderbot party mode. Ctrl+C to exit.")
    while True:
        try:
            wav = capture_phrase("main", "heard16.wav")
            heard = transcribe(wav)
            if not heard or len(re.findall(r"[A-Za-z0-9]", heard))<3 or len(heard.split())<2:
                print("[main] empty/garbled; retry."); continue

            plan = plan_action(heard)
            print("Heard:", heard)
            print("Line:", plan["say"])

            apply_plan(plan)
            speak(plan.get("say",""))

            if FAST_MODE: continue

        except KeyboardInterrupt:
            print("\n[exit]"); break
        except Exception as e:
            print("[loop err]", e); traceback.print_exc(); time.sleep(0.3)

if __name__ == "__main__":
    main()
